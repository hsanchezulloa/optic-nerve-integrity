{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Medical Image Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for 3D medical images, converting them into normalized 2D patches suitable for deep learning segmentation models.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Configuration & Setup** - Environment validation and parameter configuration\n",
    "2. **3D to 2D Conversion** - Extract valid slices from 3D volumes\n",
    "3. **Patch Extraction** - Generate fixed-size patches with quality filtering\n",
    "4. **Data Normalization** - Intensity normalization for consistent model input\n",
    "\n",
    "## Requirements\n",
    "- PyTorch, MONAI, NiBabel\n",
    "- FSL (for geometry operations)\n",
    "- Input: 3D NIfTI files organized by side (left/right)\n",
    "- Output: Normalized 32x32 patches ready for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import pipeline functions\n",
    "from functions import (\n",
    "    extract_identifier,\n",
    "    process_3d_to_2d,\n",
    "    extract_defined_patches,\n",
    "    save_patches_to_nifti,\n",
    "    reconstruct_3d_volume,\n",
    "    normalize_all_patches\n",
    ")\n",
    "\n",
    "# Configure matplotlib for inline display\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Medical Image Preprocessing Pipeline - Initialized\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 1.1 Configuration & Setup\n",
    "\n",
    "Configure all pipeline parameters in this single cell. Modify paths and parameters according to your data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS - Modify these according to your setup\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_ROOT = Path(\"data\")  # Root directory for all data\n",
    "RAW_DATA_PATH = DATA_ROOT / \"raw\"  # Contains left/ and right/ subdirectories\n",
    "PROCESSED_DATA_PATH = DATA_ROOT / \"processed\"  # Output directory\n",
    "\n",
    "# Input data structure\n",
    "INPUT_SIDES = ['left', 'right']  # Subdirectories in raw data\n",
    "IMAGE_EXTENSION = '.nii.gz'  # Expected file extension\n",
    "\n",
    "# Sample patient identifiers (replace with your actual data)\n",
    "SAMPLE_PATIENTS = ['patient001', 'patient002']  # Example patient IDs\n",
    "\n",
    "# Processing parameters\n",
    "PATCH_SIZE = (32, 32)  # Patch dimensions (height, width)\n",
    "BLACK_THRESHOLD = 0.95  # Maximum fraction of black pixels allowed in patch\n",
    "VOLUME_SHAPE = (176, 240, 165)  # Expected 3D volume dimensions (H, W, D)\n",
    "\n",
    "# Output structure\n",
    "OUTPUT_DIRS = {\n",
    "    'patches': PROCESSED_DATA_PATH / 'patches',\n",
    "    'patches_normalized': PROCESSED_DATA_PATH / 'patches_normalized',\n",
    "    'reconstructed': PROCESSED_DATA_PATH / 'reconstructed_validation'\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully:\")\n",
    "print(f\"  Raw data path: {RAW_DATA_PATH}\")\n",
    "print(f\"  Processed data path: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}\")\n",
    "print(f\"  Volume shape: {VOLUME_SHAPE}\")\n",
    "print(f\"  Expected sides: {INPUT_SIDES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "### Environment Validation\n",
    "\n",
    "Verify data structure and availability before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data structure\n",
    "def validate_data_structure() -> Dict[str, List[str]]:\n",
    "    \"\"\"Validate input data structure and return available files.\"\"\"\n",
    "    available_files = {}\n",
    "    \n",
    "    if not RAW_DATA_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Raw data directory not found: {RAW_DATA_PATH}\")\n",
    "    \n",
    "    for side in INPUT_SIDES:\n",
    "        side_path = RAW_DATA_PATH / side\n",
    "        if not side_path.exists():\n",
    "            logger.warning(f\"Side directory not found: {side_path}\")\n",
    "            available_files[side] = []\n",
    "            continue\n",
    "            \n",
    "        # Find all NIfTI files\n",
    "        files = sorted([f.name for f in side_path.glob(f'*{IMAGE_EXTENSION}')])\n",
    "        available_files[side] = files\n",
    "        \n",
    "        print(f\"Found {len(files)} files in {side}/ directory:\")\n",
    "        for file in files[:5]:  # Show first 5 files\n",
    "            patient_id = extract_identifier(file)\n",
    "            print(f\"  {file} -> Patient ID: {patient_id}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"  ... and {len(files) - 5} more files\")\n",
    "    \n",
    "    total_files = sum(len(files) for files in available_files.values())\n",
    "    print(f\"\\nTotal files found: {total_files}\")\n",
    "    \n",
    "    return available_files\n",
    "\n",
    "# Run validation\n",
    "available_files = validate_data_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversion-header",
   "metadata": {},
   "source": [
    "## 1.2 3D to 2D Conversion\n",
    "\n",
    "Convert 3D medical volumes into 2D slices, filtering out empty/black slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-volumes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3D volumes\n",
    "def load_3d_volumes(available_files: Dict[str, List[str]]) -> Dict[str, nib.Nifti1Image]:\n",
    "    \"\"\"Load all 3D volumes into memory.\"\"\"\n",
    "    volumes_3d = {}\n",
    "    \n",
    "    for side, files in available_files.items():\n",
    "        side_path = RAW_DATA_PATH / side\n",
    "        \n",
    "        for filename in files:\n",
    "            file_path = side_path / filename\n",
    "            patient_id = extract_identifier(filename)\n",
    "            key = f\"{patient_id}-{side}\"\n",
    "            \n",
    "            try:\n",
    "                volume = nib.load(str(file_path))\n",
    "                volumes_3d[key] = volume\n",
    "                logger.info(f\"Loaded {key}: {volume.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return volumes_3d\n",
    "\n",
    "# Load all volumes\n",
    "print(\"Loading 3D volumes...\")\n",
    "volumes_3d = load_3d_volumes(available_files)\n",
    "print(f\"Successfully loaded {len(volumes_3d)} volumes\")\n",
    "\n",
    "# Display volume information\n",
    "for key, volume in volumes_3d.items():\n",
    "    print(f\"  {key}: {volume.shape} - {volume.get_data_dtype()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert-to-2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 3D volumes to 2D slices\n",
    "print(\"Converting 3D volumes to 2D slices...\")\n",
    "slices_2d = process_3d_to_2d(volumes_3d)\n",
    "\n",
    "# Summary statistics\n",
    "total_valid_slices = sum(len(data['image']) for data in slices_2d.values())\n",
    "print(f\"\\nConversion complete:\")\n",
    "print(f\"  Total patients: {len(slices_2d)}\")\n",
    "print(f\"  Total valid slices: {total_valid_slices}\")\n",
    "\n",
    "# Detailed breakdown\n",
    "for patient_key, data in slices_2d.items():\n",
    "    total_slices = len(data['slices']) if data['slices'] else 0\n",
    "    valid_slices = len(data['image'])\n",
    "    print(f\"  {patient_key}: {valid_slices}/{total_slices} valid slices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "### Quality Assessment Visualization\n",
    "\n",
    "Visualize sample slices to verify conversion quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-slices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample slices\n",
    "def visualize_sample_slices(slices_2d: Dict, num_samples: int = 2) -> None:\n",
    "    \"\"\"Visualize sample slices from each patient.\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 8))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    patient_keys = list(slices_2d.keys())[:num_samples]\n",
    "    \n",
    "    for i, patient_key in enumerate(patient_keys):\n",
    "        patient_data = slices_2d[patient_key]\n",
    "        \n",
    "        if len(patient_data['image']) == 0:\n",
    "            axes[0, i].text(0.5, 0.5, 'No valid slices', ha='center', va='center')\n",
    "            axes[1, i].text(0.5, 0.5, 'No valid slices', ha='center', va='center')\n",
    "            continue\n",
    "        \n",
    "        # Show first and middle slice\n",
    "        first_slice = patient_data['image'][0].squeeze()\n",
    "        mid_idx = len(patient_data['image']) // 2\n",
    "        mid_slice = patient_data['image'][mid_idx].squeeze()\n",
    "        \n",
    "        # Plot first slice\n",
    "        axes[0, i].imshow(first_slice, cmap='gray')\n",
    "        axes[0, i].set_title(f'{patient_key}\\nFirst slice')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Plot middle slice\n",
    "        axes[1, i].imshow(mid_slice, cmap='gray')\n",
    "        axes[1, i].set_title(f'Middle slice ({mid_idx+1}/{len(patient_data[\"image\"])})')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample 2D Slices After Conversion', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if slices_2d:\n",
    "    visualize_sample_slices(slices_2d, min(2, len(slices_2d)))\n",
    "else:\n",
    "    print(\"No valid slices available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patches-header",
   "metadata": {},
   "source": [
    "## 1.3 Patch Extraction\n",
    "\n",
    "Extract fixed-size patches from 2D slices with quality filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-patches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from all slices\n",
    "def extract_all_patches(slices_2d: Dict, patch_size: Tuple[int, int], \n",
    "                       black_threshold: float) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Extract patches from all 2D slices.\"\"\"\n",
    "    patches_data = {}\n",
    "    patches_coordinates = {}\n",
    "    \n",
    "    total_patches = 0\n",
    "    \n",
    "    for patient_key, slice_data in tqdm(slices_2d.items(), desc=\"Extracting patches\"):\n",
    "        patient_patches = []\n",
    "        patient_coordinates = []\n",
    "        \n",
    "        slice_coordinates = slice_data['slices']\n",
    "        \n",
    "        for idx, image_slice in enumerate(slice_data['image']):\n",
    "            slice_num = slice_coordinates[idx] if idx < len(slice_coordinates) else idx\n",
    "            \n",
    "            # Extract patches from this slice\n",
    "            patches, coordinates = extract_defined_patches(\n",
    "                image_slice, patch_size, black_threshold, slice_num\n",
    "            )\n",
    "            \n",
    "            patient_patches.extend(patches)\n",
    "            patient_coordinates.extend(coordinates)\n",
    "        \n",
    "        patches_data[patient_key] = {'image': patient_patches}\n",
    "        patches_coordinates[patient_key] = patient_coordinates\n",
    "        \n",
    "        patient_patch_count = len(patient_patches)\n",
    "        total_patches += patient_patch_count\n",
    "        logger.info(f\"{patient_key}: {patient_patch_count} patches extracted\")\n",
    "    \n",
    "    return patches_data, patches_coordinates, total_patches\n",
    "\n",
    "# Extract patches\n",
    "print(\"Extracting patches...\")\n",
    "patches_data, patches_coordinates, total_patches = extract_all_patches(\n",
    "    slices_2d, PATCH_SIZE, BLACK_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"\\nPatch extraction complete:\")\n",
    "print(f\"  Total patches: {total_patches}\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}\")\n",
    "print(f\"  Black threshold: {BLACK_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-patches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save patches to NIfTI format\n",
    "print(\"Saving patches to NIfTI format...\")\n",
    "output_patches_dir = OUTPUT_DIRS['patches']\n",
    "\n",
    "save_patches_to_nifti(\n",
    "    patches_dir2D=patches_data,\n",
    "    patches_dir2D_coordinates=patches_coordinates,\n",
    "    base_output_dir=str(output_patches_dir)\n",
    ")\n",
    "\n",
    "print(f\"Patches saved to: {output_patches_dir}\")\n",
    "\n",
    "# Verify saved structure\n",
    "def verify_saved_structure(base_path: Path) -> None:\n",
    "    \"\"\"Verify the saved patch structure.\"\"\"\n",
    "    print(\"\\nSaved directory structure:\")\n",
    "    for side in INPUT_SIDES:\n",
    "        side_path = base_path / side\n",
    "        if side_path.exists():\n",
    "            patients = [d.name for d in side_path.iterdir() if d.is_dir()]\n",
    "            print(f\"  {side}/: {len(patients)} patients\")\n",
    "            \n",
    "            for patient in patients[:3]:  # Show first 3 patients\n",
    "                patient_path = side_path / patient\n",
    "                patch_count = len(list(patient_path.glob('*.nii.gz')))\n",
    "                print(f\"    {patient}: {patch_count} patches\")\n",
    "\nverify_saved_structure(output_patches_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-reconstruction-header",
   "metadata": {},
   "source": [
    "### Optional: Validation Reconstruction\n",
    "\n",
    "Reconstruct 3D volumes from patches to validate extraction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform validation reconstruction (optional)\n",
    "def perform_validation_reconstruction(sample_limit: int = 2) -> None:\n",
    "    \"\"\"Reconstruct 3D volumes from patches for validation.\"\"\"\n",
    "    reconstruction_dir = OUTPUT_DIRS['reconstructed']\n",
    "    patches_dir = OUTPUT_DIRS['patches']\n",
    "    \n",
    "    reconstructed_files = []\n",
    "    \n",
    "    # Get sample of patients for reconstruction\n",
    "    all_patients = set()\n",
    "    for side in INPUT_SIDES:\n",
    "        side_path = patches_dir / side\n",
    "        if side_path.exists():\n",
    "            patients = [d.name for d in side_path.iterdir() if d.is_dir()]\n",
    "            all_patients.update(patients)\n",
    "    \n",
    "    sample_patients = sorted(list(all_patients))[:sample_limit]\n",
    "    \n",
    "    for patient_id in sample_patients:\n",
    "        for side in INPUT_SIDES:\n",
    "            # Find reference image\n",
    "            reference_pattern = f\"{patient_id}_{side}*{IMAGE_EXTENSION}\"\n",
    "            reference_files = list((RAW_DATA_PATH / side).glob(reference_pattern))\n",
    "            \n",
    "            if not reference_files:\n",
    "                logger.warning(f\"No reference image found for {patient_id}-{side}\")\n",
    "                continue\n",
    "            \n",
    "            reference_path = str(reference_files[0])\n",
    "            \n",
    "            try:\n",
    "                output_path = reconstruct_3d_volume(\n",
    "                    base_patch_dir=str(patches_dir),\n",
    "                    patient_id_short=patient_id,\n",
    "                    side=side,\n",
    "                    output_base_dir=str(reconstruction_dir),\n",
    "                    reference_image_path=reference_path,\n",
    "                    volume_shape=VOLUME_SHAPE\n",
    "                )\n",
    "                reconstructed_files.append(output_path)\n",
    "                logger.info(f\"Reconstructed: {output_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Reconstruction failed for {patient_id}-{side}: {e}\")\n",
    "    \n",
    "    return reconstructed_files\n",
    "\n",
    "# Perform validation reconstruction\n",
    "print(\"Performing validation reconstruction...\")\n",
    "try:\n",
    "    reconstructed_files = perform_validation_reconstruction(sample_limit=2)\n",
    "    print(f\"Validation reconstruction complete. Files saved:\")\n",
    "    for file_path in reconstructed_files:\n",
    "        print(f\"  {file_path}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Validation reconstruction failed: {e}\")\n",
    "    print(\"Skipping validation reconstruction (optional step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalization-header",
   "metadata": {},
   "source": [
    "## 1.4 Data Normalization\n",
    "\n",
    "Apply intensity normalization to all patches for consistent model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize-patches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all patches\n",
    "def normalize_patient_patches() -> Dict[str, int]:\n",
    "    \"\"\"Normalize patches for all patients and sides.\"\"\"\n",
    "    patches_dir = OUTPUT_DIRS['patches']\n",
    "    normalized_dir = OUTPUT_DIRS['patches_normalized']\n",
    "    \n",
    "    normalization_stats = {}\n",
    "    \n",
    "    for side in INPUT_SIDES:\n",
    "        side_input_path = patches_dir / side\n",
    "        side_output_path = normalized_dir / side\n",
    "        \n",
    "        if not side_input_path.exists():\n",
    "            logger.warning(f\"Input side directory not found: {side_input_path}\")\n",
    "            continue\n",
    "        \n",
    "        patients = [d.name for d in side_input_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for patient_id in tqdm(patients, desc=f\"Normalizing {side} patches\"):\n",
    "            input_folder = side_input_path / patient_id\n",
    "            output_folder = side_output_path / patient_id\n",
    "            output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Count patches before normalization\n",
    "            input_patches = len(list(input_folder.glob('*.nii.gz')))\n",
    "            \n",
    "            try:\n",
    "                normalize_all_patches(str(input_folder), str(output_folder))\n",
    "                \n",
    "                # Count patches after normalization\n",
    "                output_patches = len(list(output_folder.glob('*.nii.gz')))\n",
    "                \n",
    "                key = f\"{patient_id}-{side}\"\n",
    "                normalization_stats[key] = {\n",
    "                    'input_patches': input_patches,\n",
    "                    'output_patches': output_patches\n",
    "                }\n",
    "                \n",
    "                logger.info(f\"Normalized {patient_id}-{side}: {input_patches} -> {output_patches} patches\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Normalization failed for {patient_id}-{side}: {e}\")\n",
    "                normalization_stats[f\"{patient_id}-{side}\"] = {'error': str(e)}\n",
    "    \n",
    "    return normalization_stats\n",
    "\n",
    "# Perform normalization\n",
    "print(\"Normalizing patches...\")\n",
    "normalization_stats = normalize_patient_patches()\n",
    "\n",
    "# Display normalization summary\n",
    "print(\"\\nNormalization complete:\")\n",
    "total_normalized = 0\n",
    "failed_normalizations = 0\n",
    "\n",
    "for key, stats in normalization_stats.items():\n",
    "    if 'error' in stats:\n",
    "        print(f\"  {key}: FAILED - {stats['error']}\")\n",
    "        failed_normalizations += 1\n",
    "    else:\n",
    "        patches_count = stats['output_patches']\n",
    "        total_normalized += patches_count\n",
    "        print(f\"  {key}: {patches_count} patches normalized\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total normalized patches: {total_normalized}\")\n",
    "print(f\"  Failed normalizations: {failed_normalizations}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIRS['patches_normalized']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-summary-header",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "Complete preprocessing pipeline execution summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pipeline summary\n",
    "def generate_pipeline_summary() -> None:\n",
    "    \"\"\"Generate and display complete pipeline summary.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MEDICAL IMAGE PREPROCESSING PIPELINE - SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Input summary\n",
    "    total_input_files = sum(len(files) for files in available_files.values())\n",
    "    print(f\"Input Data:\")\n",
    "    print(f\"  Total 3D volumes processed: {total_input_files}\")\n",
    "    print(f\"  Input directory: {RAW_DATA_PATH}\")\n",
    "    \n",
    "    # Processing summary\n",
    "    total_slices = sum(len(data['image']) for data in slices_2d.values())\n",
    "    print(f\"\\n2D Conversion:\")\n",
    "    print(f\"  Valid 2D slices extracted: {total_slices}\")\n",
    "    \n",
    "    # Patch extraction summary\n",
    "    print(f\"\\nPatch Extraction:\")\n",
    "    print(f\"  Total patches extracted: {total_patches}\")\n",
    "    print(f\"  Patch size: {PATCH_SIZE}\")\n",
    "    print(f\"  Black pixel threshold: {BLACK_THRESHOLD}\")\n",
    "    \n",
    "    # Normalization summary\n",
    "    successful_normalizations = sum(1 for stats in normalization_stats.values() if 'error' not in stats)\n",
    "    total_normalized_patches = sum(stats.get('output_patches', 0) for stats in normalization_stats.values() if 'error' not in stats)\n",
    "    \n",
    "    print(f\"\\nNormalization:\")\n",
    "    print(f\"  Successfully normalized datasets: {successful_normalizations}/{len(normalization_stats)}\")\n",
    "    print(f\"  Total normalized patches: {total_normalized_patches}\")\n",
    "    \n",
    "    # Output summary\n",
    "    print(f\"\\nOutput Directories:\")\n",
    "    for name, path in OUTPUT_DIRS.items():\n",
    "        status = \"✓\" if path.exists() else \"✗\"\n",
    "        print(f\"  {name}: {status} {path}\")\n",
    "    \n",
    "    print(f\"\\nPipeline Status: COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generate summary\n",
    "generate_pipeline_summary()\n",
    "\n",
    "# Save processing metadata\n",
    "metadata = {\n",
    "    'pipeline_version': '1.0',\n",
    "    'patch_size': PATCH_SIZE,\n",
    "    'black_threshold': BLACK_THRESHOLD,\n",
    "    'volume_shape': VOLUME_SHAPE,\n",
    "    'total_input_volumes': len(volumes_3d),\n",
    "    'total_valid_slices': sum(len(data['image']) for data in slices_2d.values()),\n",
    "    'total_patches': total_patches,\n",
    "    'normalization_stats': normalization_stats\n",
    "}\n",
    "\n",
    "# Note: In production, you might want to save this metadata to a JSON file\n",
    "print(f\"\\nMetadata available in 'metadata' variable for further processing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}