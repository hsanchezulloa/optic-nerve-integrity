{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Deep Learning Model Inference Pipeline\n",
    "\n",
    "This notebook implements the inference pipeline for optic nerve segmentation using a pre-trained 2D U-Net model.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Model Architecture** - Load and configure 2D U-Net segmentation model\n",
    "2. **Data Loading** - Prepare normalized patches for inference\n",
    "3. **Inference Pipeline** - Batch processing with performance monitoring\n",
    "4. **Results Processing** - Save predictions and generate visualizations\n",
    "\n",
    "## Requirements\n",
    "- Pre-trained model weights (model.pth)\n",
    "- Normalized patches from preprocessing pipeline\n",
    "- PyTorch, MONAI for model architecture\n",
    "- CUDA support (optional, CPU fallback available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "\n",
    "# Scientific computing and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import model and dataset functions\n",
    "from functions import (\n",
    "    SegmentationDataset,\n",
    "    create_monai_unet,\n",
    "    evaluate_model,\n",
    "    visualize_segmentation_predictions,\n",
    "    predict_and_save_masks\n",
    ")\n",
    "\n",
    "# Configure matplotlib for inline display\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Deep Learning Model Inference Pipeline - Initialized\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2.1 Configuration & Setup\n",
    "\n",
    "Configure inference parameters and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INFERENCE CONFIGURATION - Modify these according to your setup\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_ROOT = Path(\"data\")\n",
    "PROCESSED_DATA_PATH = DATA_ROOT / \"processed\"\n",
    "NORMALIZED_PATCHES_PATH = PROCESSED_DATA_PATH / \"patches_normalized\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_WEIGHTS_PATH = \"model.pth\"  # Path to pre-trained model weights\n",
    "MODEL_CONFIG = {\n",
    "    'spatial_dims': 2,\n",
    "    'in_channels': 1,\n",
    "    'out_channels': 1,\n",
    "    'channels': (16, 32, 64, 128),\n",
    "    'strides': (2, 2, 2),\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "# Inference parameters\n",
    "INFERENCE_CONFIG = {\n",
    "    'batch_size': 4,\n",
    "    'threshold': 0.5,  # Sigmoid threshold for binary predictions\n",
    "    'num_workers': 0,  # DataLoader workers (set to 0 for Windows compatibility)\n",
    "    'pin_memory': False  # Will be set based on CUDA availability\n",
    "}\n",
    "\n",
    "# Expected data structure\n",
    "INPUT_SIDES = ['left', 'right']\n",
    "SAMPLE_PATIENTS = []  # Will be auto-detected from data\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_DIRS = {\n",
    "    'predictions': PROCESSED_DATA_PATH / 'predictions',\n",
    "    'visualizations': PROCESSED_DATA_PATH / 'visualizations'\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "INFERENCE_CONFIG['pin_memory'] = torch.cuda.is_available()\n",
    "\n",
    "print(\"Inference configuration loaded:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Model weights: {MODEL_WEIGHTS_PATH}\")\n",
    "print(f\"  Batch size: {INFERENCE_CONFIG['batch_size']}\")\n",
    "print(f\"  Threshold: {INFERENCE_CONFIG['threshold']}\")\n",
    "print(f\"  Input data path: {NORMALIZED_PATCHES_PATH}\")\n",
    "print(f\"  Output directories:\")\n",
    "for name, path in OUTPUT_DIRS.items():\n",
    "    print(f\"    {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "### Environment & Data Validation\n",
    "\n",
    "Verify model weights and input data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_inference_setup() -> Dict[str, List[str]]:\n",
    "    \"\"\"Validate inference environment and data availability.\"\"\"\n",
    "    \n",
    "    # Check model weights\n",
    "    if not Path(MODEL_WEIGHTS_PATH).exists():\n",
    "        raise FileNotFoundError(f\"Model weights not found: {MODEL_WEIGHTS_PATH}\")\n",
    "    else:\n",
    "        model_size = Path(MODEL_WEIGHTS_PATH).stat().st_size / (1024 * 1024)  # MB\n",
    "        print(f\"✓ Model weights found: {MODEL_WEIGHTS_PATH} ({model_size:.1f} MB)\")\n",
    "    \n",
    "    # Check input data structure\n",
    "    if not NORMALIZED_PATCHES_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Normalized patches directory not found: {NORMALIZED_PATCHES_PATH}\")\n",
    "    \n",
    "    # Discover available data\n",
    "    available_data = {}\n",
    "    total_patches = 0\n",
    "    \n",
    "    for side in INPUT_SIDES:\n",
    "        side_path = NORMALIZED_PATCHES_PATH / side\n",
    "        if not side_path.exists():\n",
    "            logger.warning(f\"Side directory not found: {side_path}\")\n",
    "            available_data[side] = []\n",
    "            continue\n",
    "        \n",
    "        # Find patient directories\n",
    "        patient_dirs = [d.name for d in side_path.iterdir() if d.is_dir()]\n",
    "        available_data[side] = patient_dirs\n",
    "        \n",
    "        print(f\"✓ Found {len(patient_dirs)} patients in {side}/ directory\")\n",
    "        \n",
    "        # Count patches per patient\n",
    "        for patient in patient_dirs[:3]:  # Show first 3 patients\n",
    "            patient_path = side_path / patient\n",
    "            patch_count = len(list(patient_path.glob('*.nii.gz')))\n",
    "            total_patches += patch_count\n",
    "            print(f\"    {patient}: {patch_count} patches\")\n",
    "        \n",
    "        if len(patient_dirs) > 3:\n",
    "            # Count remaining patches\n",
    "            for patient in patient_dirs[3:]:\n",
    "                patient_path = side_path / patient\n",
    "                patch_count = len(list(patient_path.glob('*.nii.gz')))\n",
    "                total_patches += patch_count\n",
    "            print(f\"    ... and {len(patient_dirs) - 3} more patients\")\n",
    "    \n",
    "    print(f\"\\nTotal patches available for inference: {total_patches}\")\n",
    "    \n",
    "    return available_data\n",
    "\n",
    "# Run validation\n",
    "available_data = validate_inference_setup()\n",
    "\n",
    "# Update sample patients list\n",
    "all_patients = set()\n",
    "for patients in available_data.values():\n",
    "    all_patients.update(patients)\n",
    "SAMPLE_PATIENTS = sorted(list(all_patients))\n",
    "\n",
    "print(f\"Detected patients: {SAMPLE_PATIENTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 2.2 Model Architecture\n",
    "\n",
    "Load and configure the 2D U-Net segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load model\n",
    "def initialize_model() -> nn.Module:\n",
    "    \"\"\"Initialize the U-Net model with pre-trained weights.\"\"\"\n",
    "    \n",
    "    print(\"Creating U-Net model architecture...\")\n",
    "    model = create_monai_unet(**MODEL_CONFIG)\n",
    "    \n",
    "    # Display model architecture summary\n",
    "    def count_parameters(model):\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params\n",
    "    \n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"Model Architecture Summary:\")\n",
    "    print(f\"  Type: 2D U-Net (MONAI)\")\n",
    "    print(f\"  Input channels: {MODEL_CONFIG['in_channels']}\")\n",
    "    print(f\"  Output channels: {MODEL_CONFIG['out_channels']}\")\n",
    "    print(f\"  Feature channels: {MODEL_CONFIG['channels']}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = initialize_model()\n",
    "\n",
    "# Display detailed architecture (first few layers)\n",
    "print(\"\\nModel Architecture Details:\")\n",
    "print(model)\n",
    "\n",
    "print(f\"\\nModel successfully initialized and ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## 2.3 Data Loading\n",
    "\n",
    "Prepare datasets and data loaders for efficient batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset paths\n",
    "def prepare_inference_data() -> Tuple[List[str], SegmentationDataset]:\n",
    "    \"\"\"Prepare dataset paths and create dataset object.\"\"\"\n",
    "    \n",
    "    inference_dirs = []\n",
    "    \n",
    "    # Collect all patient directories\n",
    "    for side in INPUT_SIDES:\n",
    "        for patient in available_data.get(side, []):\n",
    "            patient_path = NORMALIZED_PATCHES_PATH / side / patient\n",
    "            if patient_path.exists():\n",
    "                inference_dirs.append(str(patient_path))\n",
    "    \n",
    "    print(f\"Prepared {len(inference_dirs)} dataset directories:\")\n",
    "    for i, dir_path in enumerate(inference_dirs):\n",
    "        patch_count = len(list(Path(dir_path).glob('*.nii.gz')))\n",
    "        dir_name = Path(dir_path).parts[-2:]  # Get last 2 parts (side/patient)\n",
    "        print(f\"  {i+1}. {'/'.join(dir_name)}: {patch_count} patches\")\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"\\nCreating inference dataset...\")\n",
    "    dataset = SegmentationDataset(inference_dirs)\n",
    "    \n",
    "    print(f\"Dataset created successfully:\")\n",
    "    print(f\"  Total patches: {len(dataset)}\")\n",
    "    print(f\"  Directories: {len(inference_dirs)}\")\n",
    "    \n",
    "    return inference_dirs, dataset\n",
    "\n",
    "# Prepare inference data\n",
    "inference_dirs, inference_dataset = prepare_inference_data()\n",
    "\n",
    "# Create data loader\n",
    "print(\"\\nCreating data loader...\")\n",
    "inference_loader = DataLoader(\n",
    "    inference_dataset,\n",
    "    batch_size=INFERENCE_CONFIG['batch_size'],\n",
    "    shuffle=False,  # Maintain order for result mapping\n",
    "    num_workers=INFERENCE_CONFIG['num_workers'],\n",
    "    pin_memory=INFERENCE_CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"Data loader configuration:\")\n",
    "print(f\"  Batch size: {INFERENCE_CONFIG['batch_size']}\")\n",
    "print(f\"  Total batches: {len(inference_loader)}\")\n",
    "print(f\"  Pin memory: {INFERENCE_CONFIG['pin_memory']}\")\n",
    "print(f\"  Workers: {INFERENCE_CONFIG['num_workers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-visualization-header",
   "metadata": {},
   "source": [
    "### Sample Data Visualization\n",
    "\n",
    "Visualize sample patches to verify data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample patches from dataset\n",
    "def visualize_sample_patches(dataset, num_samples: int = 4) -> None:\n",
    "    \"\"\"Visualize sample patches from the dataset.\"\"\"\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Select sample indices\n",
    "    indices = np.linspace(0, len(dataset) - 1, num_samples, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 4))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle('Sample Input Patches for Inference', fontsize=16)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        try:\n",
    "            # Get image from dataset\n",
    "            image = dataset[idx]\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                image_np = image.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                image_np = image.squeeze()\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(image_np, cmap='gray')\n",
    "            axes[i].set_title(f'Patch {idx}\\nShape: {image_np.shape}')\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            # Display intensity statistics\n",
    "            min_val, max_val = image_np.min(), image_np.max()\n",
    "            mean_val = image_np.mean()\n",
    "            axes[i].text(0.02, 0.98, f'Range: [{min_val:.3f}, {max_val:.3f}]\\nMean: {mean_val:.3f}', \n",
    "                        transform=axes[i].transAxes, fontsize=8, \n",
    "                        verticalalignment='top', color='white',\n",
    "                        bbox=dict(boxstyle='round', facecolor='black', alpha=0.5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            axes[i].text(0.5, 0.5, f'Error loading\\nindex {idx}', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].axis('off')\n",
    "            logger.error(f\"Error visualizing sample {idx}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "if len(inference_dataset) > 0:\n",
    "    visualize_sample_patches(inference_dataset, min(4, len(inference_dataset)))\n",
    "else:\n",
    "    print(\"No patches available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## 2.4 Model Inference\n",
    "\n",
    "Run inference on all patches with performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model evaluation\n",
    "def run_inference_evaluation() -> Dict:\n",
    "    \"\"\"Run model inference and return evaluation results.\"\"\"\n",
    "    \n",
    "    print(\"Starting model inference...\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Model weights: {MODEL_WEIGHTS_PATH}\")\n",
    "    print(f\"Threshold: {INFERENCE_CONFIG['threshold']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run evaluation (inference only, no ground truth masks)\n",
    "    results = evaluate_model(\n",
    "        model=model,\n",
    "        test_loader=inference_loader,\n",
    "        model_weights_path=MODEL_WEIGHTS_PATH,\n",
    "        threshold=INFERENCE_CONFIG['threshold'],\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_patches = len(inference_dataset)\n",
    "    patches_per_second = total_patches / inference_time if inference_time > 0 else 0\n",
    "    time_per_batch = inference_time / results['batches'] if results['batches'] > 0 else 0\n",
    "    \n",
    "    results.update({\n",
    "        'total_patches': total_patches,\n",
    "        'inference_time': inference_time,\n",
    "        'patches_per_second': patches_per_second,\n",
    "        'time_per_batch': time_per_batch\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run inference\n",
    "inference_results = run_inference_evaluation()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nInference Results:\")\n",
    "print(f\"  Total patches processed: {inference_results['total_patches']}\")\n",
    "print(f\"  Total batches: {inference_results['batches']}\")\n",
    "print(f\"  Inference time: {inference_results['inference_time']:.2f} seconds\")\n",
    "print(f\"  Processing speed: {inference_results['patches_per_second']:.2f} patches/second\")\n",
    "print(f\"  Average time per batch: {inference_results['time_per_batch']:.4f} seconds\")\n",
    "\n",
    "# Performance assessment\n",
    "if inference_results['patches_per_second'] > 50:\n",
    "    print(\"  Performance: ✓ Excellent processing speed\")\n",
    "elif inference_results['patches_per_second'] > 20:\n",
    "    print(\"  Performance: ✓ Good processing speed\")\n",
    "else:\n",
    "    print(\"  Performance: ⚠ Consider optimizing batch size or using GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions-header",
   "metadata": {},
   "source": [
    "### Prediction Visualization\n",
    "\n",
    "Visualize model predictions on sample patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "print(\"Generating prediction visualizations...\")\n",
    "\n",
    "try:\n",
    "    # Generate visualization with sample predictions\n",
    "    visualize_segmentation_predictions(\n",
    "        model=model,\n",
    "        test_dataset=inference_dataset,\n",
    "        num_images=4\n",
    "    )\n",
    "    \n",
    "    print(\"Prediction visualization completed successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Visualization failed: {e}\")\n",
    "    print(f\"Error during visualization: {e}\")\n",
    "    print(\"Continuing with prediction saving...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-predictions-header",
   "metadata": {},
   "source": [
    "## 2.5 Save Predictions\n",
    "\n",
    "Save all predictions as NIfTI files with structured organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all predictions\n",
    "def save_inference_results() -> List[str]:\n",
    "    \"\"\"Save all model predictions as NIfTI files.\"\"\"\n",
    "    \n",
    "    print(\"Saving prediction results...\")\n",
    "    print(f\"Output directory: {OUTPUT_DIRS['predictions']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Save predictions\n",
    "    saved_paths = predict_and_save_masks(\n",
    "        model=model,\n",
    "        test_dataset=inference_dataset,\n",
    "        test_image_dirs=inference_dirs,\n",
    "        output_base_dir=str(OUTPUT_DIRS['predictions']),\n",
    "        threshold=INFERENCE_CONFIG['threshold'],\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    save_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nPrediction saving completed:\")\n",
    "    print(f\"  Total predictions saved: {len(saved_paths)}\")\n",
    "    print(f\"  Save time: {save_time:.2f} seconds\")\n",
    "    print(f\"  Average time per prediction: {save_time/len(saved_paths):.4f} seconds\")\n",
    "    \n",
    "    return saved_paths\n",
    "\n",
    "# Save predictions\n",
    "saved_prediction_paths = save_inference_results()\n",
    "\n",
    "# Verify saved structure\n",
    "def verify_prediction_structure(output_dir: Path) -> Dict[str, int]:\n",
    "    \"\"\"Verify the structure of saved predictions.\"\"\"\n",
    "    structure_info = {}\n",
    "    \n",
    "    print(\"\\nSaved prediction structure:\")\n",
    "    \n",
    "    for side in INPUT_SIDES:\n",
    "        side_path = output_dir / side\n",
    "        if side_path.exists():\n",
    "            patients = [d.name for d in side_path.iterdir() if d.is_dir()]\n",
    "            total_predictions = 0\n",
    "            \n",
    "            print(f\"  {side}/: {len(patients)} patients\")\n",
    "            \n",
    "            for patient in sorted(patients):\n",
    "                patient_path = side_path / patient\n",
    "                pred_count = len(list(patient_path.glob('*.nii.gz')))\n",
    "                total_predictions += pred_count\n",
    "                print(f\"    {patient}: {pred_count} predictions\")\n",
    "            \n",
    "            structure_info[side] = total_predictions\n",
    "        else:\n",
    "            print(f\"  {side}/: No predictions found\")\n",
    "            structure_info[side] = 0\n",
    "    \n",
    "    return structure_info\n",
    "\n",
    "# Verify structure\n",
    "prediction_structure = verify_prediction_structure(OUTPUT_DIRS['predictions'])\n",
    "total_saved = sum(prediction_structure.values())\n",
    "\n",
    "print(f\"\\nTotal predictions saved across all sides: {total_saved}\")\n",
    "\n",
    "# Validation check\n",
    "if total_saved == len(saved_prediction_paths):\n",
    "    print(\"✓ Prediction count validation passed\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: Mismatch in prediction counts ({total_saved} vs {len(saved_prediction_paths)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "Complete inference pipeline execution summary and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive pipeline summary\n",
    "def generate_inference_summary() -> Dict:\n",
    "    \"\"\"Generate and display complete inference pipeline summary.\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'inference_config': INFERENCE_CONFIG,\n",
    "        'device': str(DEVICE),\n",
    "        'data_summary': {\n",
    "            'total_patients': len(SAMPLE_PATIENTS),\n",
    "            'total_patches': len(inference_dataset),\n",
    "            'total_directories': len(inference_dirs)\n",
    "        },\n",
    "        'inference_results': inference_results,\n",
    "        'output_summary': {\n",
    "            'total_predictions': len(saved_prediction_paths),\n",
    "            'predictions_by_side': prediction_structure,\n",
    "            'output_directory': str(OUTPUT_DIRS['predictions'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEEP LEARNING INFERENCE PIPELINE - SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Model summary\n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"  Architecture: 2D U-Net (MONAI)\")\n",
    "    print(f\"  Weights: {MODEL_WEIGHTS_PATH}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    print(f\"  Input/Output channels: {MODEL_CONFIG['in_channels']} -> {MODEL_CONFIG['out_channels']}\")\n",
    "    \n",
    "    # Data summary\n",
    "    print(f\"\\nData Processing:\")\n",
    "    print(f\"  Patients processed: {len(SAMPLE_PATIENTS)}\")\n",
    "    print(f\"  Total patches: {len(inference_dataset)}\")\n",
    "    print(f\"  Batch size: {INFERENCE_CONFIG['batch_size']}\")\n",
    "    print(f\"  Total batches: {inference_results['batches']}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Inference time: {inference_results['inference_time']:.2f} seconds\")\n",
    "    print(f\"  Processing speed: {inference_results['patches_per_second']:.2f} patches/second\")\n",
    "    print(f\"  Average batch time: {inference_results['time_per_batch']:.4f} seconds\")\n",
    "    \n",
    "    # Output summary\n",
    "    print(f\"\\nOutput Results:\")\n",
    "    print(f\"  Total predictions saved: {len(saved_prediction_paths)}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIRS['predictions']}\")\n",
    "    print(f\"  Predictions by side:\")\n",
    "    for side, count in prediction_structure.items():\n",
    "        print(f\"    {side}: {count} predictions\")\n",
    "    \n",
    "    # Quality checks\n",
    "    print(f\"\\nQuality Checks:\")\n",
    "    checks_passed = 0\n",
    "    total_checks = 3\n",
    "    \n",
    "    if len(saved_prediction_paths) > 0:\n",
    "        print(f\"  ✓ Predictions generated successfully\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"  ✗ No predictions generated\")\n",
    "    \n",
    "    if len(saved_prediction_paths) == len(inference_dataset):\n",
    "        print(f\"  ✓ All input patches processed\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"  ⚠ Patch count mismatch: {len(saved_prediction_paths)} vs {len(inference_dataset)}\")\n",
    "    \n",
    "    if inference_results['patches_per_second'] > 10:\n",
    "        print(f\"  ✓ Acceptable processing speed\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"  ⚠ Processing speed below optimal\")\n",
    "    \n",
    "    # Final status\n",
    "    if checks_passed == total_checks:\n",
    "        status = \"COMPLETED SUCCESSFULLY\"\n",
    "    elif checks_passed >= total_checks - 1:\n",
    "        status = \"COMPLETED WITH WARNINGS\"\n",
    "    else:\n",
    "        status = \"COMPLETED WITH ISSUES\"\n",
    "    \n",
    "    print(f\"\\nPipeline Status: {status} ({checks_passed}/{total_checks} checks passed)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "pipeline_summary = generate_inference_summary()\n",
    "\n",
    "print(f\"\\nInference pipeline completed successfully!\")\n",
    "print(f\"Predictions are ready for 3D reconstruction in the next notebook.\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Run 03_reconstruction_analysis.ipynb for 3D volume reconstruction\")\n",
    "print(f\"  2. Analyze segmentation quality and results\")\n",
    "print(f\"  3. Export final results for clinical analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}