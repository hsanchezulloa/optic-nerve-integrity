{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 3D Volume Reconstruction & Analysis Pipeline\n",
    "\n",
    "This notebook reconstructs 3D segmented volumes from predicted 2D patches and provides comprehensive analysis tools for optic nerve segmentation results.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Volume Reconstruction** - Assemble 3D volumes from predicted patches\n",
    "2. **Quality Assessment** - Validate reconstruction accuracy and completeness\n",
    "3. **Results Analysis** - Generate quantitative and qualitative analysis\n",
    "4. **Export & Visualization** - Prepare final results for clinical use\n",
    "\n",
    "## Requirements\n",
    "- Predicted segmentation masks from inference pipeline\n",
    "- Original reference images for geometry preservation\n",
    "- FSL tools for geometry copying (optional)\n",
    "- Sufficient disk space for 3D volume outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Scientific computing and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import nibabel as nib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Import reconstruction functions\n",
    "from functions import reconstruct_3d_volume\n",
    "\n",
    "# Configure matplotlib for inline display\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"3D Volume Reconstruction & Analysis Pipeline - Initialized\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NiBabel version: {nib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3.1 Configuration & Setup\n",
    "\n",
    "Configure reconstruction parameters and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RECONSTRUCTION CONFIGURATION - Modify these according to your setup\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_ROOT = Path(\"data\")\n",
    "PROCESSED_DATA_PATH = DATA_ROOT / \"processed\"\n",
    "RAW_DATA_PATH = DATA_ROOT / \"raw\"\n",
    "\n",
    "# Input data paths\n",
    "PREDICTIONS_PATH = PROCESSED_DATA_PATH / \"predictions\"  # From inference pipeline\n",
    "REFERENCE_IMAGES_PATH = RAW_DATA_PATH  # Original images for geometry\n",
    "\n",
    "# Volume parameters\n",
    "VOLUME_CONFIG = {\n",
    "    'volume_shape': (176, 240, 165),  # Expected 3D volume dimensions (H, W, D)\n",
    "    'patch_size': 32,  # Patch size used during extraction\n",
    "    'image_extension': '.nii.gz'  # File extension for medical images\n",
    "}\n",
    "\n",
    "# Processing parameters\n",
    "INPUT_SIDES = ['left', 'right']\n",
    "SAMPLE_PATIENTS = []  # Will be auto-detected from prediction data\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_DIRS = {\n",
    "    'reconstructed': PROCESSED_DATA_PATH / 'reconstructed_volumes',\n",
    "    'analysis': PROCESSED_DATA_PATH / 'analysis_results',\n",
    "    'visualizations': PROCESSED_DATA_PATH / 'volume_visualizations',\n",
    "    'exports': PROCESSED_DATA_PATH / 'final_exports'\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'connectivity_threshold': 0.5,  # Threshold for connected components\n",
    "    'min_component_size': 10,  # Minimum size for valid components\n",
    "    'slice_sampling': 5,  # Every nth slice for visualization\n",
    "    'visualization_slices': 10  # Number of slices to show in overview\n",
    "}\n",
    "\n",
    "# Create all output directories\n",
    "for dir_name, path in OUTPUT_DIRS.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"  {dir_name}: {path}\")\n",
    "\n",
    "print(\"\\nConfiguration loaded successfully:\")\n",
    "print(f\"  Volume shape: {VOLUME_CONFIG['volume_shape']}\")\n",
    "print(f\"  Patch size: {VOLUME_CONFIG['patch_size']}\")\n",
    "print(f\"  Predictions path: {PREDICTIONS_PATH}\")\n",
    "print(f\"  Reference images: {REFERENCE_IMAGES_PATH}\")\n",
    "print(f\"  Output directories created: {len(OUTPUT_DIRS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "Validate input predictions and reference images availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_reconstruction_data() -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"Validate prediction data and reference images availability.\"\"\"\n",
    "    \n",
    "    validation_data = {\n",
    "        'predictions': {},\n",
    "        'references': {}\n",
    "    }\n",
    "    \n",
    "    # Check predictions directory\n",
    "    if not PREDICTIONS_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Predictions directory not found: {PREDICTIONS_PATH}\")\n",
    "    \n",
    "    print(\"Validating prediction data...\")\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for side in INPUT_SIDES:\n",
    "        side_pred_path = PREDICTIONS_PATH / side\n",
    "        if not side_pred_path.exists():\n",
    "            logger.warning(f\"Prediction side directory not found: {side_pred_path}\")\n",
    "            validation_data['predictions'][side] = []\n",
    "            continue\n",
    "        \n",
    "        # Find patient directories with predictions\n",
    "        patients = [d.name for d in side_pred_path.iterdir() if d.is_dir()]\n",
    "        validation_data['predictions'][side] = patients\n",
    "        \n",
    "        print(f\"  {side}: {len(patients)} patients with predictions\")\n",
    "        \n",
    "        for patient in patients[:3]:  # Show details for first 3 patients\n",
    "            patient_path = side_pred_path / patient\n",
    "            pred_count = len(list(patient_path.glob('*.nii.gz')))\n",
    "            total_predictions += pred_count\n",
    "            print(f\"    {patient}: {pred_count} prediction patches\")\n",
    "        \n",
    "        if len(patients) > 3:\n",
    "            for patient in patients[3:]:\n",
    "                patient_path = side_pred_path / patient\n",
    "                pred_count = len(list(patient_path.glob('*.nii.gz')))\n",
    "                total_predictions += pred_count\n",
    "            print(f\"    ... and {len(patients) - 3} more patients\")\n",
    "    \n",
    "    print(f\"Total prediction patches: {total_predictions}\")\n",
    "    \n",
    "    # Check reference images\n",
    "    print(\"\\nValidating reference images...\")\n",
    "    total_references = 0\n",
    "    \n",
    "    if REFERENCE_IMAGES_PATH.exists():\n",
    "        for side in INPUT_SIDES:\n",
    "            side_ref_path = REFERENCE_IMAGES_PATH / side\n",
    "            if side_ref_path.exists():\n",
    "                ref_files = list(side_ref_path.glob(f'*{VOLUME_CONFIG[\"image_extension\"]}'))\n",
    "                ref_patients = [f.stem.split('_')[0] for f in ref_files]\n",
    "                validation_data['references'][side] = ref_patients\n",
    "                total_references += len(ref_files)\n",
    "                print(f\"  {side}: {len(ref_files)} reference images\")\n",
    "            else:\n",
    "                validation_data['references'][side] = []\n",
    "                logger.warning(f\"Reference side directory not found: {side_ref_path}\")\n",
    "    else:\n",
    "        logger.warning(f\"Reference images directory not found: {REFERENCE_IMAGES_PATH}\")\n",
    "        for side in INPUT_SIDES:\n",
    "            validation_data['references'][side] = []\n",
    "    \n",
    "    print(f\"Total reference images: {total_references}\")\n",
    "    \n",
    "    # Determine available patients for reconstruction\n",
    "    all_pred_patients = set()\n",
    "    for patients in validation_data['predictions'].values():\n",
    "        all_pred_patients.update(patients)\n",
    "    \n",
    "    global SAMPLE_PATIENTS\n",
    "    SAMPLE_PATIENTS = sorted(list(all_pred_patients))\n",
    "    \n",
    "    print(f\"\\nPatients available for reconstruction: {SAMPLE_PATIENTS}\")\n",
    "    \n",
    "    return validation_data\n",
    "\n",
    "# Run validation\n",
    "validation_data = validate_reconstruction_data()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nValidation summary:\")\n",
    "print(f\"  Patients with predictions: {len(SAMPLE_PATIENTS)}\")\n",
    "print(f\"  Sides available: {INPUT_SIDES}\")\n",
    "print(f\"  Ready for reconstruction: ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reconstruction-header",
   "metadata": {},
   "source": [
    "## 3.2 Volume Reconstruction\n",
    "\n",
    "Reconstruct 3D volumes from predicted segmentation patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reconstruct-volumes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_all_volumes() -> Dict[str, str]:\n",
    "    \"\"\"Reconstruct 3D volumes for all available patients and sides.\"\"\"\n",
    "    \n",
    "    reconstructed_files = {}\n",
    "    reconstruction_stats = {\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'total_time': 0\n",
    "    }\n",
    "    \n",
    "    print(\"Starting 3D volume reconstruction...\")\n",
    "    print(f\"Output directory: {OUTPUT_DIRS['reconstructed']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for patient_id in tqdm(SAMPLE_PATIENTS, desc=\"Reconstructing volumes\"):\n",
    "        for side in INPUT_SIDES:\n",
    "            # Check if predictions exist for this patient-side combination\n",
    "            pred_path = PREDICTIONS_PATH / side / patient_id\n",
    "            if not pred_path.exists():\n",
    "                logger.warning(f\"No predictions found for {patient_id}-{side}\")\n",
    "                continue\n",
    "            \n",
    "            # Find reference image\n",
    "            reference_path = None\n",
    "            ref_side_path = REFERENCE_IMAGES_PATH / side\n",
    "            \n",
    "            if ref_side_path.exists():\n",
    "                # Look for reference image matching patient ID\n",
    "                ref_pattern = f\"{patient_id}_*{VOLUME_CONFIG['image_extension']}\"\n",
    "                ref_files = list(ref_side_path.glob(ref_pattern))\n",
    "                \n",
    "                if ref_files:\n",
    "                    reference_path = str(ref_files[0])\n",
    "                else:\n",
    "                    # Try alternative naming patterns\n",
    "                    alt_patterns = [\n",
    "                        f\"{patient_id}*{side}*{VOLUME_CONFIG['image_extension']}\",\n",
    "                        f\"*{patient_id}*{VOLUME_CONFIG['image_extension']}\"\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in alt_patterns:\n",
    "                        ref_files = list(ref_side_path.glob(pattern))\n",
    "                        if ref_files:\n",
    "                            reference_path = str(ref_files[0])\n",
    "                            break\n",
    "            \n",
    "            if not reference_path:\n",
    "                logger.warning(f\"No reference image found for {patient_id}-{side}\")\n",
    "                logger.info(f\"Proceeding without geometry copying for {patient_id}-{side}\")\n",
    "                # Create a dummy reference path - the function will handle missing files gracefully\n",
    "                reference_path = str(ref_side_path / f\"{patient_id}_dummy.nii.gz\")\n",
    "            \n",
    "            try:\n",
    "                # Perform reconstruction\n",
    "                output_path = reconstruct_3d_volume(\n",
    "                    base_patch_dir=str(PREDICTIONS_PATH),\n",
    "                    patient_id_short=patient_id,\n",
    "                    side=side,\n",
    "                    output_base_dir=str(OUTPUT_DIRS['reconstructed']),\n",
    "                    reference_image_path=reference_path,\n",
    "                    volume_shape=VOLUME_CONFIG['volume_shape'],\n",
    "                    patch_size=VOLUME_CONFIG['patch_size']\n",
    "                )\n",
    "                \n",
    "                key = f\"{patient_id}-{side}\"\n",
    "                reconstructed_files[key] = output_path\n",
    "                reconstruction_stats['successful'] += 1\n",
    "                \n",
    "                logger.info(f\"✓ Reconstructed {key}: {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                reconstruction_stats['failed'] += 1\n",
    "                logger.error(f\"✗ Reconstruction failed for {patient_id}-{side}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    end_time = time.time()\n",
    "    reconstruction_stats['total_time'] = end_time - start_time\n",
    "    \n",
    "    return reconstructed_files, reconstruction_stats\n",
    "\n",
    "# Perform reconstruction\n",
    "reconstructed_files, reconstruction_stats = reconstruct_all_volumes()\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nReconstruction completed:\")\n",
    "print(f\"  Successful: {reconstruction_stats['successful']}\")\n",
    "print(f\"  Failed: {reconstruction_stats['failed']}\")\n",
    "print(f\"  Total time: {reconstruction_stats['total_time']:.2f} seconds\")\n",
    "print(f\"  Average time per volume: {reconstruction_stats['total_time']/max(1, reconstruction_stats['successful']):.2f} seconds\")\n",
    "\n",
    "print(f\"\\nReconstructed volumes:\")\n",
    "for key, path in reconstructed_files.items():\n",
    "    file_size = Path(path).stat().st_size / (1024 * 1024) if Path(path).exists() else 0\n",
    "    print(f\"  {key}: {Path(path).name} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-header",
   "metadata": {},
   "source": [
    "## 3.3 Quality Assessment\n",
    "\n",
    "Analyze reconstruction quality and segmentation characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_volume_quality(volume_path: str) -> Dict:\n",
    "    \"\"\"Analyze quality metrics for a reconstructed volume.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load volume\n",
    "        volume_img = nib.load(volume_path)\n",
    "        volume_data = volume_img.get_fdata()\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            'shape': volume_data.shape,\n",
    "            'dtype': str(volume_data.dtype),\n",
    "            'file_size_mb': Path(volume_path).stat().st_size / (1024 * 1024),\n",
    "            'min_value': float(volume_data.min()),\n",
    "            'max_value': float(volume_data.max()),\n",
    "            'mean_value': float(volume_data.mean()),\n",
    "            'std_value': float(volume_data.std())\n",
    "        }\n",
    "        \n",
    "        # Segmentation-specific metrics\n",
    "        if volume_data.max() <= 1.0:  # Binary or probability mask\n",
    "            # Binarize if needed\n",
    "            binary_mask = (volume_data >= ANALYSIS_CONFIG['connectivity_threshold']).astype(np.uint8)\n",
    "            \n",
    "            stats.update({\n",
    "                'total_voxels': int(volume_data.size),\n",
    "                'positive_voxels': int(binary_mask.sum()),\n",
    "                'positive_ratio': float(binary_mask.sum() / volume_data.size),\n",
    "                'non_zero_slices': int(np.sum(binary_mask.sum(axis=(0, 1)) > 0))\n",
    "            })\n",
    "            \n",
    "            # Connected components analysis\n",
    "            labeled_array, num_components = ndimage.label(binary_mask)\n",
    "            \n",
    "            if num_components > 0:\n",
    "                component_sizes = [np.sum(labeled_array == i) for i in range(1, num_components + 1)]\n",
    "                stats.update({\n",
    "                    'num_components': num_components,\n",
    "                    'largest_component': max(component_sizes) if component_sizes else 0,\n",
    "                    'component_sizes': component_sizes[:10]  # Top 10 components\n",
    "                })\n",
    "            else:\n",
    "                stats.update({\n",
    "                    'num_components': 0,\n",
    "                    'largest_component': 0,\n",
    "                    'component_sizes': []\n",
    "                })\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Quality analysis failed for {volume_path}: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Analyze all reconstructed volumes\n",
    "def perform_quality_assessment() -> Dict[str, Dict]:\n",
    "    \"\"\"Perform quality assessment on all reconstructed volumes.\"\"\"\n",
    "    \n",
    "    quality_results = {}\n",
    "    \n",
    "    print(\"Performing quality assessment...\")\n",
    "    \n",
    "    for key, volume_path in tqdm(reconstructed_files.items(), desc=\"Analyzing quality\"):\n",
    "        if not Path(volume_path).exists():\n",
    "            logger.warning(f\"Volume file not found: {volume_path}\")\n",
    "            continue\n",
    "        \n",
    "        quality_stats = analyze_volume_quality(volume_path)\n",
    "        quality_results[key] = quality_stats\n",
    "    \n",
    "    return quality_results\n",
    "\n",
    "# Run quality assessment\n",
    "quality_results = perform_quality_assessment()\n",
    "\n",
    "# Display quality summary\n",
    "print(f\"\\nQuality Assessment Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, stats in quality_results.items():\n",
    "    if 'error' in stats:\n",
    "        print(f\"\\n{key}: ERROR - {stats['error']}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Volume shape: {stats['shape']}\")\n",
    "    print(f\"  File size: {stats['file_size_mb']:.1f} MB\")\n",
    "    print(f\"  Value range: [{stats['min_value']:.3f}, {stats['max_value']:.3f}]\")\n",
    "    \n",
    "    if 'positive_voxels' in stats:\n",
    "        print(f\"  Segmented voxels: {stats['positive_voxels']:,} ({stats['positive_ratio']:.1%})\")\n",
    "        print(f\"  Active slices: {stats['non_zero_slices']}/{stats['shape'][2]}\")\n",
    "        print(f\"  Connected components: {stats['num_components']}\")\n",
    "        if stats['largest_component'] > 0:\n",
    "            print(f\"  Largest component: {stats['largest_component']:,} voxels\")\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "valid_results = [stats for stats in quality_results.values() if 'error' not in stats]\n",
    "\n",
    "if valid_results:\n",
    "    avg_file_size = np.mean([stats['file_size_mb'] for stats in valid_results])\n",
    "    avg_positive_ratio = np.mean([stats.get('positive_ratio', 0) for stats in valid_results])\n",
    "    \n",
    "    print(f\"\\nAggregate Statistics:\")\n",
    "    print(f\"  Average file size: {avg_file_size:.1f} MB\")\n",
    "    print(f\"  Average segmentation ratio: {avg_positive_ratio:.1%}\")\n",
    "    print(f\"  Successfully analyzed: {len(valid_results)}/{len(quality_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "## 3.4 Results Visualization\n",
    "\n",
    "Generate comprehensive visualizations of reconstructed volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volume-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_volume_slices(volume_path: str, patient_key: str, \n",
    "                           num_slices: int = 6) -> None:\n",
    "    \"\"\"Visualize sample slices from a 3D volume.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load volume\n",
    "        volume_img = nib.load(volume_path)\n",
    "        volume_data = volume_img.get_fdata()\n",
    "        \n",
    "        # Find slices with content\n",
    "        slice_sums = volume_data.sum(axis=(0, 1))\n",
    "        active_slices = np.where(slice_sums > 0)[0]\n",
    "        \n",
    "        if len(active_slices) == 0:\n",
    "            print(f\"No active slices found in {patient_key}\")\n",
    "            return\n",
    "        \n",
    "        # Select representative slices\n",
    "        if len(active_slices) >= num_slices:\n",
    "            slice_indices = np.linspace(0, len(active_slices)-1, num_slices, dtype=int)\n",
    "            selected_slices = active_slices[slice_indices]\n",
    "        else:\n",
    "            selected_slices = active_slices\n",
    "        \n",
    "        # Create visualization\n",
    "        cols = min(3, len(selected_slices))\n",
    "        rows = (len(selected_slices) + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        fig.suptitle(f'Volume Reconstruction: {patient_key}', fontsize=16)\n",
    "        \n",
    "        for i, slice_idx in enumerate(selected_slices):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            \n",
    "            if rows == 1:\n",
    "                ax = axes[col] if cols > 1 else axes[0]\n",
    "            else:\n",
    "                ax = axes[row, col] if cols > 1 else axes[row, 0]\n",
    "            \n",
    "            slice_data = volume_data[:, :, slice_idx]\n",
    "            \n",
    "            # Display slice\n",
    "            im = ax.imshow(slice_data, cmap='hot', interpolation='nearest')\n",
    "            ax.set_title(f'Slice {slice_idx}\\nSum: {slice_sums[slice_idx]:.0f}')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(selected_slices), rows * cols):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            if rows == 1:\n",
    "                ax = axes[col] if cols > 1 else axes[0]\n",
    "            else:\n",
    "                ax = axes[row, col] if cols > 1 else axes[row, 0]\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        vis_path = OUTPUT_DIRS['visualizations'] / f'{patient_key}_slices.png'\n",
    "        plt.savefig(vis_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Visualization saved: {vis_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Visualization failed for {patient_key}: {e}\")\n",
    "\n",
    "# Generate visualizations for all volumes\n",
    "def generate_all_visualizations(max_volumes: int = 4) -> None:\n",
    "    \"\"\"Generate visualizations for reconstructed volumes.\"\"\"\n",
    "    \n",
    "    print(f\"Generating visualizations...\")\n",
    "    \n",
    "    volume_keys = list(reconstructed_files.keys())[:max_volumes]\n",
    "    \n",
    "    for key in volume_keys:\n",
    "        volume_path = reconstructed_files[key]\n",
    "        if Path(volume_path).exists():\n",
    "            print(f\"\\nVisualizing {key}...\")\n",
    "            visualize_volume_slices(volume_path, key, num_slices=6)\n",
    "        else:\n",
    "            logger.warning(f\"Volume not found for visualization: {volume_path}\")\n",
    "\n",
    "# Generate visualizations\n",
    "if reconstructed_files:\n",
    "    generate_all_visualizations(max_volumes=min(4, len(reconstructed_files)))\n",
    "else:\n",
    "    print(\"No reconstructed volumes available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-summary-header",
   "metadata": {},
   "source": [
    "### Segmentation Analysis Summary\n",
    "\n",
    "Comprehensive analysis of segmentation results across all volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segmentation_summary() -> Dict:\n",
    "    \"\"\"Generate comprehensive segmentation analysis summary.\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'volume_analysis': {},\n",
    "        'aggregate_metrics': {},\n",
    "        'quality_flags': {}\n",
    "    }\n",
    "    \n",
    "    valid_volumes = [key for key, stats in quality_results.items() \n",
    "                    if 'error' not in stats and 'positive_voxels' in stats]\n",
    "    \n",
    "    if not valid_volumes:\n",
    "        print(\"No valid volumes found for analysis\")\n",
    "        return summary\n",
    "    \n",
    "    print(\"Generating segmentation analysis summary...\")\n",
    "    \n",
    "    # Per-volume analysis\n",
    "    for key in valid_volumes:\n",
    "        stats = quality_results[key]\n",
    "        \n",
    "        volume_summary = {\n",
    "            'segmentation_volume_mm3': stats['positive_voxels'],  # Assuming 1mm³ voxels\n",
    "            'segmentation_ratio': stats['positive_ratio'],\n",
    "            'active_slices': stats['non_zero_slices'],\n",
    "            'connectivity': stats['num_components'],\n",
    "            'largest_component_ratio': stats['largest_component'] / max(1, stats['positive_voxels'])\n",
    "        }\n",
    "        \n",
    "        # Quality flags\n",
    "        flags = []\n",
    "        if stats['positive_ratio'] < 0.001:  # Less than 0.1% segmented\n",
    "            flags.append('low_segmentation')\n",
    "        if stats['positive_ratio'] > 0.1:  # More than 10% segmented (unusual for optic nerve)\n",
    "            flags.append('high_segmentation')\n",
    "        if stats['num_components'] > 5:\n",
    "            flags.append('fragmented')\n",
    "        if stats['non_zero_slices'] < 5:\n",
    "            flags.append('sparse_slices')\n",
    "        \n",
    "        volume_summary['quality_flags'] = flags\n",
    "        summary['volume_analysis'][key] = volume_summary\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    all_ratios = [summary['volume_analysis'][key]['segmentation_ratio'] for key in valid_volumes]\n",
    "    all_volumes = [quality_results[key]['positive_voxels'] for key in valid_volumes]\n",
    "    all_components = [quality_results[key]['num_components'] for key in valid_volumes]\n",
    "    all_slices = [quality_results[key]['non_zero_slices'] for key in valid_volumes]\n",
    "    \n",
    "    summary['aggregate_metrics'] = {\n",
    "        'mean_segmentation_ratio': np.mean(all_ratios),\n",
    "        'std_segmentation_ratio': np.std(all_ratios),\n",
    "        'mean_volume': np.mean(all_volumes),\n",
    "        'std_volume': np.std(all_volumes),\n",
    "        'mean_components': np.mean(all_components),\n",
    "        'mean_active_slices': np.mean(all_slices)\n",
    "    }\n",
    "    \n",
    "    # Overall quality assessment\n",
    "    all_flags = [flag for vol in summary['volume_analysis'].values() \n",
    "                for flag in vol['quality_flags']]\n",
    "    flag_counts = {flag: all_flags.count(flag) for flag in set(all_flags)}\n",
    "    summary['quality_flags'] = flag_counts\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "segmentation_summary = generate_segmentation_summary()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSEGMENTATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if segmentation_summary['aggregate_metrics']:\n",
    "    metrics = segmentation_summary['aggregate_metrics']\n",
    "    print(f\"\\nAggregate Metrics ({len(segmentation_summary['volume_analysis'])} volumes):\")\n",
    "    print(f\"  Mean segmentation ratio: {metrics['mean_segmentation_ratio']:.3%} ± {metrics['std_segmentation_ratio']:.3%}\")\n",
    "    print(f\"  Mean segmented volume: {metrics['mean_volume']:.0f} ± {metrics['std_volume']:.0f} voxels\")\n",
    "    print(f\"  Mean connected components: {metrics['mean_components']:.1f}\")\n",
    "    print(f\"  Mean active slices: {metrics['mean_active_slices']:.1f}\")\n",
    "\n",
    "if segmentation_summary['quality_flags']:\n",
    "    print(f\"\\nQuality Flags:\")\n",
    "    for flag, count in segmentation_summary['quality_flags'].items():\n",
    "        print(f\"  {flag}: {count} volumes\")\n",
    "else:\n",
    "    print(f\"\\n✓ No quality issues detected\")\n",
    "\n",
    "print(f\"\\nPer-Volume Analysis:\")\n",
    "for key, analysis in segmentation_summary['volume_analysis'].items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Segmentation ratio: {analysis['segmentation_ratio']:.3%}\")\n",
    "    print(f\"  Volume: {analysis['segmentation_volume_mm3']} voxels\")\n",
    "    print(f\"  Active slices: {analysis['active_slices']}\")\n",
    "    print(f\"  Components: {analysis['connectivity']}\")\n",
    "    if analysis['quality_flags']:\n",
    "        print(f\"  Quality flags: {', '.join(analysis['quality_flags'])}\")\n",
    "    else:\n",
    "        print(f\"  Quality: ✓ No issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 3.5 Results Export\n",
    "\n",
    "Export analysis results and prepare final deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results() -> Dict[str, str]:\n",
    "    \"\"\"Export all analysis results to structured files.\"\"\"\n",
    "    \n",
    "    export_files = {}\n",
    "    \n",
    "    print(\"Exporting analysis results...\")\n",
    "    \n",
    "    # 1. Export quality assessment as JSON\n",
    "    quality_export = {\n",
    "        'quality_results': quality_results,\n",
    "        'segmentation_summary': segmentation_summary,\n",
    "        'reconstruction_stats': reconstruction_stats,\n",
    "        'configuration': {\n",
    "            'volume_shape': VOLUME_CONFIG['volume_shape'],\n",
    "            'patch_size': VOLUME_CONFIG['patch_size'],\n",
    "            'analysis_config': ANALYSIS_CONFIG\n",
    "        },\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    quality_file = OUTPUT_DIRS['exports'] / 'quality_assessment.json'\n",
    "    with open(quality_file, 'w') as f:\n",
    "        json.dump(quality_export, f, indent=2, default=str)\n",
    "    export_files['quality_assessment'] = str(quality_file)\n",
    "    print(f\"  Quality assessment: {quality_file}\")\n",
    "    \n",
    "    # 2. Export file inventory\n",
    "    file_inventory = {\n",
    "        'reconstructed_volumes': {key: str(Path(path).name) for key, path in reconstructed_files.items()},\n",
    "        'volume_paths': reconstructed_files,\n",
    "        'original_predictions': str(PREDICTIONS_PATH),\n",
    "        'output_directories': {name: str(path) for name, path in OUTPUT_DIRS.items()}\n",
    "    }\n",
    "    \n",
    "    inventory_file = OUTPUT_DIRS['exports'] / 'file_inventory.json'\n",
    "    with open(inventory_file, 'w') as f:\n",
    "        json.dump(file_inventory, f, indent=2)\n",
    "    export_files['file_inventory'] = str(inventory_file)\n",
    "    print(f\"  File inventory: {inventory_file}\")\n",
    "    \n",
    "    # 3. Create summary report\n",
    "    report_content = f\"\"\"# 3D Volume Reconstruction Analysis Report\n",
    "\n",
    "Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Pipeline Summary\n",
    "\n",
    "### Input Data\n",
    "- Predictions source: {PREDICTIONS_PATH}\n",
    "- Reference images: {REFERENCE_IMAGES_PATH}\n",
    "- Patients processed: {len(SAMPLE_PATIENTS)}\n",
    "- Sides: {', '.join(INPUT_SIDES)}\n",
    "\n",
    "### Reconstruction Results\n",
    "- Successful reconstructions: {reconstruction_stats['successful']}\n",
    "- Failed reconstructions: {reconstruction_stats['failed']}\n",
    "- Total processing time: {reconstruction_stats['total_time']:.2f} seconds\n",
    "- Volume shape: {VOLUME_CONFIG['volume_shape']}\n",
    "- Patch size: {VOLUME_CONFIG['patch_size']}\n",
    "\n",
    "### Quality Metrics\n",
    "\"\"\"\n",
    "    \n",
    "    if segmentation_summary['aggregate_metrics']:\n",
    "        metrics = segmentation_summary['aggregate_metrics']\n",
    "        report_content += f\"\"\"\n",
    "- Mean segmentation ratio: {metrics['mean_segmentation_ratio']:.3%}\n",
    "- Mean segmented volume: {metrics['mean_volume']:.0f} voxels\n",
    "- Mean connected components: {metrics['mean_components']:.1f}\n",
    "- Mean active slices: {metrics['mean_active_slices']:.1f}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "\n",
    "### Output Files\n",
    "\n",
    "#### Reconstructed Volumes\n",
    "\"\"\"\n",
    "    \n",
    "    for key, path in reconstructed_files.items():\n",
    "        file_size = Path(path).stat().st_size / (1024 * 1024) if Path(path).exists() else 0\n",
    "        report_content += f\"- {key}: {Path(path).name} ({file_size:.1f} MB)\\n\"\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "\n",
    "#### Analysis Files\n",
    "- Quality assessment: quality_assessment.json\n",
    "- File inventory: file_inventory.json\n",
    "- Volume visualizations: {OUTPUT_DIRS['visualizations']}\n",
    "\n",
    "#### Output Directories\n",
    "\"\"\"\n",
    "    \n",
    "    for name, path in OUTPUT_DIRS.items():\n",
    "        report_content += f\"- {name}: {path}\\n\"\n",
    "    \n",
    "    report_file = OUTPUT_DIRS['exports'] / 'reconstruction_report.md'\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    export_files['reconstruction_report'] = str(report_file)\n",
    "    print(f\"  Reconstruction report: {report_file}\")\n",
    "    \n",
    "    return export_files\n",
    "\n",
    "# Export results\n",
    "exported_files = export_analysis_results()\n",
    "\n",
    "print(f\"\\nExport completed successfully!\")\n",
    "print(f\"All results saved to: {OUTPUT_DIRS['exports']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-summary-header",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "Complete 3D reconstruction and analysis pipeline summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final pipeline summary\n",
    "def generate_final_summary() -> None:\n",
    "    \"\"\"Generate comprehensive final summary of the entire pipeline.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"3D VOLUME RECONSTRUCTION & ANALYSIS PIPELINE - FINAL SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Input summary\n",
    "    print(f\"\\n📊 INPUT DATA SUMMARY\")\n",
    "    print(f\"  Patients processed: {len(SAMPLE_PATIENTS)}\")\n",
    "    print(f\"  Anatomical sides: {', '.join(INPUT_SIDES)}\")\n",
    "    print(f\"  Predictions source: {PREDICTIONS_PATH}\")\n",
    "    print(f\"  Reference images: {REFERENCE_IMAGES_PATH}\")\n",
    "    \n",
    "    # Reconstruction summary\n",
    "    print(f\"\\n🔧 RECONSTRUCTION SUMMARY\")\n",
    "    print(f\"  Successful reconstructions: {reconstruction_stats['successful']}\")\n",
    "    print(f\"  Failed reconstructions: {reconstruction_stats['failed']}\")\n",
    "    print(f\"  Success rate: {reconstruction_stats['successful']/(reconstruction_stats['successful']+reconstruction_stats['failed'])*100:.1f}%\")\n",
    "    print(f\"  Processing time: {reconstruction_stats['total_time']:.2f} seconds\")\n",
    "    print(f\"  Volume dimensions: {VOLUME_CONFIG['volume_shape']}\")\n",
    "    \n",
    "    # Quality summary\n",
    "    valid_analyses = len([r for r in quality_results.values() if 'error' not in r])\n",
    "    print(f\"\\n🎯 QUALITY ASSESSMENT\")\n",
    "    print(f\"  Volumes analyzed: {valid_analyses}/{len(quality_results)}\")\n",
    "    \n",
    "    if segmentation_summary['aggregate_metrics']:\n",
    "        metrics = segmentation_summary['aggregate_metrics']\n",
    "        print(f\"  Mean segmentation coverage: {metrics['mean_segmentation_ratio']:.3%}\")\n",
    "        print(f\"  Average segmented volume: {metrics['mean_volume']:.0f} voxels\")\n",
    "        print(f\"  Average connectivity: {metrics['mean_components']:.1f} components\")\n",
    "    \n",
    "    # Quality flags summary\n",
    "    if segmentation_summary['quality_flags']:\n",
    "        print(f\"\\n⚠️  QUALITY FLAGS\")\n",
    "        for flag, count in segmentation_summary['quality_flags'].items():\n",
    "            print(f\"  {flag.replace('_', ' ').title()}: {count} volumes\")\n",
    "    else:\n",
    "        print(f\"\\n✅ QUALITY STATUS: No issues detected\")\n",
    "    \n",
    "    # Output summary\n",
    "    print(f\"\\n📁 OUTPUT SUMMARY\")\n",
    "    print(f\"  Reconstructed volumes: {len(reconstructed_files)}\")\n",
    "    print(f\"  Analysis files exported: {len(exported_files)}\")\n",
    "    print(f\"  Visualizations generated: {len(list(OUTPUT_DIRS['visualizations'].glob('*.png')))}\")\n",
    "    \n",
    "    total_size = 0\n",
    "    for path in reconstructed_files.values():\n",
    "        if Path(path).exists():\n",
    "            total_size += Path(path).stat().st_size\n",
    "    \n",
    "    print(f\"  Total output size: {total_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # File locations\n",
    "    print(f\"\\n📂 KEY OUTPUT LOCATIONS\")\n",
    "    for name, path in OUTPUT_DIRS.items():\n",
    "        file_count = len(list(path.rglob('*'))) if path.exists() else 0\n",
    "        print(f\"  {name.title()}: {path} ({file_count} items)\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    if reconstruction_stats['successful'] > 0:\n",
    "        avg_time = reconstruction_stats['total_time'] / reconstruction_stats['successful']\n",
    "        print(f\"\\n⚡ PERFORMANCE METRICS\")\n",
    "        print(f\"  Average reconstruction time: {avg_time:.2f} seconds/volume\")\n",
    "        \n",
    "        if avg_time < 30:\n",
    "            perf_status = \"Excellent\"\n",
    "        elif avg_time < 60:\n",
    "            perf_status = \"Good\"\n",
    "        else:\n",
    "            perf_status = \"Acceptable\"\n",
    "        \n",
    "        print(f\"  Performance rating: {perf_status}\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\n🚀 RECOMMENDED NEXT STEPS\")\n",
    "    print(f\"  1. Review quality flags and investigate any issues\")\n",
    "    print(f\"  2. Validate results against clinical expectations\")\n",
    "    print(f\"  3. Use reconstructed volumes for further analysis\")\n",
    "    print(f\"  4. Consider parameter optimization if quality is suboptimal\")\n",
    "    \n",
    "    # Success indicator\n",
    "    if (reconstruction_stats['successful'] > 0 and \n",
    "        valid_analyses == len(quality_results) and\n",
    "        len(exported_files) > 0):\n",
    "        status = \"🎉 PIPELINE COMPLETED SUCCESSFULLY\"\n",
    "    elif reconstruction_stats['successful'] > 0:\n",
    "        status = \"⚠️ PIPELINE COMPLETED WITH WARNINGS\"\n",
    "    else:\n",
    "        status = \"❌ PIPELINE COMPLETED WITH ERRORS\"\n",
    "    \n",
    "    print(f\"\\n{status}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Generate final summary\n",
    "generate_final_summary()\n",
    "\n",
    "# Final message\n",
    "print(f\"\\n🔬 3D Volume Reconstruction & Analysis Pipeline Complete!\")\n",
    "print(f\"📊 All results, visualizations, and analysis reports are ready for review.\")\n",
    "print(f\"📁 Check the output directories for detailed results and exported files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}